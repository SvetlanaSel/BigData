# ะะฐะฑะพัะฐัะพัะฝะฐั ัะฐะฑะพัะฐ 5.1. ะะฝะฐะปะธะท ะดะฐะฝะฝัั ะทะตะผะปะตััััะตะฝะธะน ั ะธัะฟะพะปัะทะพะฒะฐะฝะธะตะผ Hadoop

[![Docker](https://img.shields.io/badge/Docker-Ready-blue)]()
[![Hadoop](https://img.shields.io/badge/Hadoop-3.3.4-green)]()
[![Python](https://img.shields.io/badge/Python-3.8+-blue)]()

## ๐ ะะพััะฐะฝะพะฒะบะฐ ะทะฐะดะฐัะธ

**ะฆะตะปั.** ะะพะปััะธัั ะฟัะฐะบัะธัะตัะบะธะต ะฝะฐะฒัะบะธ ัะฐะทะฒะตัััะฒะฐะฝะธั ะพะดะฝะพัะทะปะพะฒะพะณะพ ะบะปะฐััะตัะฐ Hadoop, ะพัะฒะพะธัั ะฑะฐะทะพะฒัะต ะพะฟะตัะฐัะธะธ ั ัะฐัะฟัะตะดะตะปะตะฝะฝะพะน ัะฐะนะปะพะฒะพะน ัะธััะตะผะพะน HDFS, ะฒัะฟะพะปะฝะธัั ะทะฐะณััะทะบั ะธ ะพะฑัะฐะฑะพัะบั ะดะฐะฝะฝัั, ะฐ ัะฐะบะถะต ะฟัะพะฐะฝะฐะปะธะทะธัะพะฒะฐัั ะธ ะฒะธะทัะฐะปะธะทะธัะพะฒะฐัั ัะตะทัะปััะฐัั.

**ะะฝะฐะปะธัะธัะตัะบะฐั ะทะฐะดะฐัะฐ.** ะะฟัะตะดะตะปะธัั ัะธะฟ ะทะตะผะปะตััััะตะฝะธั ั ะผะฐะบัะธะผะฐะปัะฝะพะน ััะตะดะฝะตะน ะผะฐะณะฝะธััะดะพะน ะธะท ะดะฐัะฐัะตัะฐ ะธััะพัะธัะตัะบะธั ะดะฐะฝะฝัั (ะฒะฐัะธะฐะฝั 30).

**ะััะพัะฝะธะบ ะดะฐะฝะฝัั.** https://www.kaggle.com/datasets/usgs/earthquake-database

---

## ๐ ะััะธัะตะบัััะฐ ัะธััะตะผั

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ              Docker Container (Ubuntu 20.04)             โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโค
โ                                                          โ
โ  โโโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโโ    โโโโโโโโโโโโ    โ
โ  โ  NameNode    โ    โ   DataNode   โ    โResource  โ    โ
โ  โ  (HDFS)      โโโโโบโ   (HDFS)     โ    โ Manager  โ    โ
โ  โโโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโโ    โโโโโโโโโโโโ    โ
โ         โ                                             โ  โ
โ         โ                                             โ  โ
โ  โโโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโโ    โโโโโโโโโโโโ    โ
โ  โ Secondary    โ    โ   DataNode   โ    โ   Node   โ    โ
โ  โ   NameNode   โ    โ   (HDFS)     โ    โ Manager  โ    โ
โ  โโโโโโโโโโโโโโโโ    โโโโโโโโโโโโโโโโ    โโโโโโโโโโโโ    โ
โ                                                          โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ    โ
โ  โ           HDFS File System                       โ    โ
โ  โ    /user/hadoop/input/database.csv               โ    โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ    โ
โ                                                          โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ    โ
โ  โ         Python / PySpark Analysis                โ    โ
โ  โ    โข Pandas ะดะปั ะฑััััะพะณะพ ะฐะฝะฐะปะธะทะฐ                 โ    โ
โ  โ    โข PySpark ะดะปั ะฑะพะปััะธั ะดะฐะฝะฝัั                  โ    โ
โ  โ    โข Jupyter ะดะปั ะฒะธะทัะฐะปะธะทะฐัะธะธ                    โ    โ
โ  โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ    โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
                โ
                โ HTTP
                โผ
        http://localhost:9870 (HDFS UI)
        http://localhost:8088 (YARN UI)
```

---

## ๐ง ะขะตัะฝะพะปะพะณะธัะตัะบะธะน ััะตะบ

| ะขะตัะฝะพะปะพะณะธั | ะะตััะธั | ะะฐะทะฝะฐัะตะฝะธะต |
|-----------|--------|-----------|
| **Hadoop** | 3.3.4 | ะะฐัะฟัะตะดะตะปะตะฝะฝะฐั ะพะฑัะฐะฑะพัะบะฐ ะดะฐะฝะฝัั |
| **HDFS** | 3.3.4 | ะะฐัะฟัะตะดะตะปะตะฝะฝะฐั ัะฐะนะปะพะฒะฐั ัะธััะตะผะฐ |
| **YARN** | 3.3.4 | ะฃะฟัะฐะฒะปะตะฝะธะต ัะตััััะฐะผะธ ะบะปะฐััะตัะฐ |
| **Python** | 3.8+ | ะฏะทัะบ ะฟัะพะณัะฐะผะผะธัะพะฒะฐะฝะธั |
| **Pandas** | 2.0+ | ะะฝะฐะปะธะท ะธ ะพะฑัะฐะฑะพัะบะฐ ะดะฐะฝะฝัั |
| **PySpark** | 3.4+ | ะะฑัะฐะฑะพัะบะฐ ะฑะพะปััะธั ะดะฐะฝะฝัั |
| **Jupyter** | latest | ะะฝัะตัะฐะบัะธะฒะฝัะน ะฐะฝะฐะปะธะท |
| **Matplotlib** | 3.7+ | ะะธะทัะฐะปะธะทะฐัะธั ะดะฐะฝะฝัั |
| **Docker** | latest | ะะพะฝัะตะนะฝะตัะธะทะฐัะธั ะพะบััะถะตะฝะธั |
| **Ubuntu** | 20.04 | ะะฐะทะพะฒะฐั ะะก ะบะพะฝัะตะนะฝะตัะฐ |

---

## ๐ ะะฐะฟััะบ ะฟัะพะตะบัะฐ

### ะจะฐะณ 1. ะะปะพะฝะธัะพะฒะฐะฝะธะต ะธ ะฟะพะดะณะพัะพะฒะบะฐ

```bash
# ะะปะพะฝะธัะพะฒะฐัั ัะตะฟะพะทะธัะพัะธะน
git clone <repository_url>
cd lw_5_1

# ะฃะฑะตะดะธัััั, ััะพ ัะฐะนะปั ะฝะฐ ะผะตััะต
ls -la hadoop/
ls -la scripts/
ls -la notebooks/
```

**ะัะพะฒะตัะบะฐ:** ะะพะปะถะฝั ัะฒะธะดะตัั 6 ัะฐะนะปะพะฒ ะฒ ะดะธัะตะบัะพัะธะธ `hadoop/`:
- `workers`
- `core-site.xml`
- `hdfs-site.xml`
- `yarn-site.xml`
- `mapred-site.xml`
- `log4j.properties`

### ะจะฐะณ 2. ะะฐะฟััะบ Docker ะบะพะฝัะตะนะฝะตัะฐ

```bash
# ะะฐะฟัััะธัั ะบะพะฝัะตะนะฝะตั
# docker compose up --build # if new data rewrite database.csv
docker compose up -d

# ะัะพัะผะพััะตัั ะปะพะณะธ (ะฟะพะดะพะถะดะธัะต 60-90 ัะตะบัะฝะด)
docker compose logs -f hadoop

# ะะถะธะดะฐะตะผัะน ะฒัะฒะพะด ะฒ ะบะพะฝัะต:
# === ะกัะฐััั ัะตัะฒะธัะพะฒ ===
# [ะดะพะปะถะฝั ะฑััั: NameNode, DataNode, SecondaryNameNode, ResourceManager, NodeManager]
```

**ะัะพะฒะตัะบะฐ:** ะ ะปะพะณะฐั ะฝะต ะดะพะปะถะฝะพ ะฑััั ะพัะธะฑะพะบ "JAVA_HOME is not set" ะธะปะธ "Connection refused".

### ะจะฐะณ 3. ะะพะดะบะปััะตะฝะธะต ะบ ะบะพะฝัะตะนะฝะตัั

```bash
# ะัะบัััั ัะตัะผะธะฝะฐะป ะฒะฝัััะธ ะบะพะฝัะตะนะฝะตัะฐ
docker compose exec hadoop bash

# ะัะพะฒะตัะธัั, ััะพ ะฒั ะฒะฝัััะธ ะบะพะฝัะตะนะฝะตัะฐ
hostname  # ะดะพะปะถะตะฝ ะฟะพะบะฐะทะฐัั: hadoop
```

### ะจะฐะณ 4. ะัะพะฒะตัะบะฐ ะบะพะผะฟะพะฝะตะฝัะพะฒ Hadoop

```bash
# ะัะพะฒะตัะธัั ััะฐััั ะฒัะตั ะฟัะพัะตััะพะฒ
jps

# ะะพะปะถะฝั ัะฒะธะดะตัั:
# - NameNode
# - DataNode
# - SecondaryNameNode
# - ResourceManager
# - NodeManager
# - Jps
```

**ะัะพะฒะตัะบะฐ ัะตะทัะปััะฐัะฐ.** ะฃะฑะตะดะธัะตัั, ััะพ ะฒัะต 5 ะฟัะพัะตััะพะฒ ะทะฐะฟััะตะฝั (NameNode, DataNode, SecondaryNameNode, ResourceManager, NodeManager).

---

## ๐ ะะฐะฑะพัะฐ ั HDFS

### ะจะฐะณ 1. ะกะพะทะดะฐะฝะธะต ะดะธัะตะบัะพัะธะน

```bash
# ะกะพะทะดะฐัั ะดะธัะตะบัะพัะธะธ ะดะปั ะฒัะพะดะฝัั ะธ ะฒััะพะดะฝัั ะดะฐะฝะฝัั
hdfs dfs -mkdir -p /user/hadoop/input
hdfs dfs -mkdir -p /user/hadoop/output

# ะัะพะฒะตัะธัั ัะพะทะดะฐะฝะฝัะต ะดะธัะตะบัะพัะธะธ
hdfs dfs -ls /user/hadoop/
```

**ะัะพะฒะตัะบะฐ.** ะะพะปะถะฝั ัะฒะธะดะตัั ะดะธัะตะบัะพัะธะธ `input` ะธ `output`.

### ะจะฐะณ 2. ะะฐะณััะทะบะฐ ะดะฐะฝะฝัั

```bash
# ะะฐะณััะทะธัั dataset ะฒ HDFS
hdfs dfs -put /opt/data/database.csv /user/hadoop/input/database.csv

# ะัะพะฒะตัะธัั ะทะฐะณััะทะบั
hdfs dfs -ls -h /user/hadoop/input/

# ะัะพัะผะพััะตัั ัะฐะทะผะตั ัะฐะนะปะฐ
hdfs dfs -du -h /user/hadoop/input/
```

**ะัะพะฒะตัะบะฐ.** ะคะฐะนะป `database.csv` ะดะพะปะถะตะฝ ะฑััั ะฒ HDFS. ะั ัะฒะธะดะธัะต ะตะณะพ ัะฐะทะผะตั (~5-10 MB).

### ะจะฐะณ 3. ะัะพัะผะพัั ะดะฐะฝะฝัั ะฒ HDFS

```bash
# ะัะพัะผะพััะตัั ะฟะตัะฒัะต ัััะพะบะธ ัะฐะนะปะฐ ะธะท HDFS
hdfs dfs -cat /user/hadoop/input/database.csv | head -20

# ะัะพะฒะตัะธัั ััะฐัะธััะธะบั HDFS
hdfs dfsadmin -report
```

**ะัะพะฒะตัะบะฐ.** ะะพะปะถะฝั ัะฒะธะดะตัั ะทะฐะณะพะปะพะฒะบะธ CSV: `Date,Time,Latitude,Longitude,Type,Depth,Depth Error...`

### ะจะฐะณ 4. ะะตะฑ-ะธะฝัะตััะตะนัั (ะพัะบัััั ะฒ ะฑัะฐัะทะตัะต)

```bash
# ะัะบัััั ะฒ ะฑัะฐัะทะตัะต (ะฝะฐ ัะพััะต, ะฝะต ะฒ ะบะพะฝัะตะนะฝะตัะต):
```

- **HDFS NameNode UI:** http://localhost:9870
  - ะะฐะฒะธะณะฐัะธั. Browse the file system โ `/user/hadoop/input/` โ `database.csv`
  
- **YARN ResourceManager UI:** http://localhost:8088

**ะัะพะฒะตัะบะฐ.** ะ ะฑัะฐัะทะตัะต ะดะพะปะถะฝั ะพัะบัััััั ะฒะตะฑ-ะธะฝัะตััะตะนัั Hadoop. ะ HDFS UI ะผะพะถะฝะพ ัะฒะธะดะตัั ะทะฐะณััะถะตะฝะฝัะน ัะฐะนะป.

---

## ๐ ะะฝะฐะปะธะท ะดะฐะฝะฝัั

### ะะฐัะธะฐะฝั 1. Pandas (ะฑัััััะน ะฐะฝะฐะปะธะท)

```bash
cd /opt/scripts

# ะะฐะฟัััะธัั ะฐะฝะฐะปะธะท
python3 analyze_pandas.py

# ะัะพะฒะตัะธัั ัะตะทัะปััะฐั
cat ../results/magnitude_by_type.csv
head -10 ../results/magnitude_by_type.csv
```

**ะัะพะฒะตัะบะฐ ัะตะทัะปััะฐัะฐ:** 
- ะะพะปะถะตะฝ ะฒัะฒะตััะธัั ัะธะฟ ะทะตะผะปะตััััะตะฝะธั ั ะผะฐะบัะธะผะฐะปัะฝะพะน ััะตะดะฝะตะน ะผะฐะณะฝะธััะดะพะน
- ะคะฐะนะป `results/magnitude_by_type.csv` ะดะพะปะถะตะฝ ะฑััั ัะพะทะดะฐะฝ
- ะัะพัะผะพััะธัะต ะฟะตัะฒัะต ัััะพะบะธ CSV ัะฐะนะปะฐ

### ะะฐัะธะฐะฝั 2. PySpark (ะดะปั ะฑะพะปััะธั ะดะฐะฝะฝัั)

```bash
cd /opt/scripts

# ะะฐะฟัััะธัั ะฐะฝะฐะปะธะท ัะตัะตะท Spark
python3 analyze_spark.py

# ะัะพะฒะตัะธัั ัะตะทัะปััะฐัั ะฒ HDFS
hdfs dfs -ls /user/hadoop/output
hdfs dfs -cat /user/hadoop/output/magnitude_by_type/part-00000 | head -20
```

**ะัะพะฒะตัะบะฐ ัะตะทัะปััะฐัะฐ.** ะะตะทัะปััะฐัั ะดะพะปะถะฝั ะฑััั ะฒ HDFS ะฒ ะดะธัะตะบัะพัะธะธ `/user/hadoop/output/`.

### ะะฐัะธะฐะฝั 3. Jupyter Notebook (ะฒะธะทัะฐะปะธะทะฐัะธั)

```bash
# ะะฐะฟัััะธัั Jupyter ะฑะตะท ัะพะบะตะฝะฐ (ัะดะพะฑะฝะพ ะดะปั ัะฐะทัะฐะฑะพัะบะธ)
cd /opt
bash scripts/start_jupyter.sh

# ะัะบัััั ะฑัะฐัะทะตั: http://localhost:8888
# (ะดะพัััะฟ ะฑะตะท ัะพะบะตะฝะฐ - ะพัะบััะฒะฐะตััั ััะฐะทั)

# ะัะบัััั notebook: notebooks/earthquake_analysis.ipynb
# ะัะฟะพะปะฝะธัั ะฒัะต ััะตะนะบะธ (Run All)
```

**ะัะพะฒะตัะบะฐ:** 
- Jupyter ะดะพะปะถะตะฝ ะพัะบัััััั ะฒ ะฑัะฐัะทะตัะต ะฟะพ ะฐะดัะตัั http://localhost:8888
- ะัะฟะพะปะฝะธัะต ะบะพะผะฐะฝะดั `hdfs dfs -ls /user/hadoop/input` ะฒ ะฟะตัะฒะพะน ััะตะนะบะต - ะดะพะปะถะตะฝ ะฟะพะบะฐะทะฐัั ะทะฐะณััะถะตะฝะฝัะน ัะฐะนะป
- ะัะฟะพะปะฝะธัั ะฒัะต ััะตะนะบะธ ะฒ notebook (Run โ Run All)
- ะะพัะผะพััะตัั ะณัะฐัะธะบะธ ะธ ัะตะทัะปััะฐัั ะฐะฝะฐะปะธะทะฐ ะฒ ะดะธัะตะบัะพัะธะธ `results/`

---

## ๐ ะะถะธะดะฐะตะผัะน ัะตะทัะปััะฐั

ะะพัะปะต ะฒัะฟะพะปะฝะตะฝะธั ะฐะฝะฐะปะธะทะฐ ะฒั ะฟะพะปััะธัะต:

1. **ะขะธะฟ ะทะตะผะปะตััััะตะฝะธั ั ะผะฐะบัะธะผะฐะปัะฝะพะน ััะตะดะฝะตะน ะผะฐะณะฝะธััะดะพะน**
2. **ะขะฐะฑะปะธัั ัะตะทัะปััะฐัะพะฒ** - `results/magnitude_by_type.csv` ั ัะฐัะฟัะตะดะตะปะตะฝะธะตะผ ะฟะพ ัะธะฟะฐะผ
3. **ะะธะทัะฐะปะธะทะฐัะธะธ**:
   - ะัะฐัะธะบ ััะตะดะฝะตะน ะผะฐะณะฝะธััะดั ะฟะพ ัะธะฟะฐะผ (ัะพะฟ-10)
   - ะะฐัะฟัะตะดะตะปะตะฝะธะต ะบะพะปะธัะตััะฒะฐ ะทะตะผะปะตััััะตะฝะธะน vs ะผะฐะณะฝะธััะดะฐ
   - Box plot ัะฐัะฟัะตะดะตะปะตะฝะธั ะผะฐะณะฝะธััะด
4. **ะัะฒะพะดั ะธ ะธะฝัะตัะฟัะตัะฐัะธั** ัะตะทัะปััะฐัะพะฒ

### ะัะธะผะตั ะฟัะพะฒะตัะบะธ ัะตะทัะปััะฐัะฐ

```bash
# ะัะพัะผะพััะตัั ัะตะทัะปััะฐัั
cat results/magnitude_by_type.csv

# ะัะธะผะตัะฝัะน ะฒัะฒะพะด:
# Type,Mean_Magnitude,Count
# Nuclear Explosion,5.85,7
# Explosion,5.32,124
# ...
```

**ะัะพะฒะตัะบะฐ.** ะัะพัะผะพััะธัะต ัะฐะฑะปะธัั - ะฟะตัะฒัะน ัะธะฟ ะฒ ัะฟะธัะบะต ะดะพะปะถะตะฝ ะธะผะตัั ะผะฐะบัะธะผะฐะปัะฝัั ััะตะดะฝัั ะผะฐะณะฝะธััะดั.

---

## ๐ ะกัััะบัััะฐ ะพััะตัะฐ

ะััะตั ะดะพะปะถะตะฝ ะฑััั ะทะฐะณััะถะตะฝ ะฒ Git-ัะตะฟะพะทะธัะพัะธะน ะธ ะฒะบะปััะฐัั:

### 1. README.md (ััะพั ัะฐะนะป)
- ะะฟะธัะฐะฝะธะต ะฟัะพะตะบัะฐ
- ะะฝััััะบัะธะธ ะฟะพ ะทะฐะฟััะบั
- ะะตะทัะปััะฐัั ะฐะฝะฐะปะธะทะฐ

### 2. ะััะพะดะฝัะน ะบะพะด
```
notebooks/
โโโ earthquake_analysis.ipynb  # Jupyter notebook ั ะฐะฝะฐะปะธะทะพะผ
scripts/
โโโ analyze_pandas.py         # Pandas ะฐะฝะฐะปะธะท
โโโ analyze_spark.py         # PySpark ะฐะฝะฐะปะธะท
```

### 3. ะะตะทัะปััะฐัั ะฐะฝะฐะปะธะทะฐ
```
results/
โโโ magnitude_by_type.csv        # ะขะฐะฑะปะธัะฐ ัะตะทัะปััะฐัะพะฒ
โโโ magnitude_by_type.png        # ะัะฐัะธะบ
โโโ report.txt                   # ะขะตะบััะพะฒัะน ะพััะตั
```

### 4. ะะพะฝัะธะณััะฐัะธั
```
hadoop/                          # ะะพะฝัะธะณััะฐัะธั Hadoop
Dockerfile                       # Docker ะพะฑัะฐะท
docker-compose.yml              # Docker Compose ะบะพะฝัะธะณััะฐัะธั
requirements.txt                 # Python ะทะฐะฒะธัะธะผะพััะธ
```

### 5. ะกะบัะธะฝัะพัั ะฒะตะฑ-ะธะฝัะตััะตะนัะพะฒ
- HDFS NameNode UI ั ะทะฐะณััะถะตะฝะฝัะผะธ ัะฐะนะปะฐะผะธ
- YARN ResourceManager UI
- Jupyter ั ะฒะธะทัะฐะปะธะทะฐัะธัะผะธ

---

## ๐ ะัะธัะตัะธะธ ะพัะตะฝะบะธ (10 ะฑะฐะปะปะพะฒ)

| ะัะธัะตัะธะน | ะะฐะปะปั | ะะฟะธัะฐะฝะธะต |
|----------|-------|----------|
| **ะะฐัััะพะนะบะฐ Hadoop** | 2 | ะะพััะตะบัะฝะพะต ัะฐะทะฒะตัััะฒะฐะฝะธะต, ะฒัะต ัะตัะฒะธัั ะทะฐะฟััะตะฝั |
| **ะะฐะณััะทะบะฐ ะฒ HDFS** | 1 | ะะฐะฝะฝัะต ะทะฐะณััะถะตะฝั ะธ ะฒะธะดะฝั ะฒ ะฒะตะฑ-ะธะฝัะตััะตะนัะต |
| **ะะตัะตะฝะธะต ัะตัะตะท Hadoop** | 2 | ะะฝะฐะปะธะท ะฒัะฟะพะปะฝะตะฝ ั ะธัะฟะพะปัะทะพะฒะฐะฝะธะตะผ HDFS |
| **ะะตัะตะฝะธะต ัะตัะตะท Spark** | 2 | ะะฝะฐะปะธะท ะฒัะฟะพะปะฝะตะฝ ัะตัะตะท PySpark |
| **ะะธะทัะฐะปะธะทะฐัะธั** | 2 | ะะฐัะตััะฒะตะฝะฝัะต ะณัะฐัะธะบะธ ะธ ะฒะธะทัะฐะปะธะทะฐัะธะธ |
| **ะััะตั ะธ ะพัะพัะผะปะตะฝะธะต** | 1 | ะะพะปะฝัะน ะพััะตั ะฒ Git-ัะตะฟะพะทะธัะพัะธะธ |

**ะขัะตะฑะพะฒะฐะฝะธั ะบ ะพััะตัั:**
- ะขะธััะปัะฝัะน ะปะธัั ั ะฒะฐัะธะฐะฝัะพะผ ะทะฐะดะฐะฝะธั
- ะะพััะฐะฝะพะฒะบะฐ ะทะฐะดะฐัะธ
- ะััะธัะตะบัััะฐ ัะธััะตะผั
- ะขะตัะฝะพะปะพะณะธัะตัะบะธะน ััะตะบ
- ะะพัะฐะณะพะฒะพะต ัะตัะตะฝะธะต ั ะฟัะพะฒะตัะบะฐะผะธ
- ะะตะทัะปััะฐัั ะฐะฝะฐะปะธะทะฐ ั ะณัะฐัะธะบะฐะผะธ
- ะะฐะบะปััะตะฝะธะต
- ะกะฟะธัะพะบ ะปะธัะตัะฐัััั

---

## ๐ ะฃะฟัะฐะฒะปะตะฝะธะต ะฟัะพะตะบัะพะผ

### ะะตัะตัะฑะพัะบะฐ ะฟัะพะตะบัะฐ

```bash
# ะััะฐะฝะพะฒะธัั ะบะพะฝัะตะนะฝะตั
docker compose down

# ะะตัะตัะพะฑัะฐัั ะพะฑัะฐะท
docker compose build --no-cache

# ะะฐะฟัััะธัั ะทะฐะฝะพะฒะพ
docker compose up -d

# ะัะพะฒะตัะธัั ััะฐััั
docker compose logs -f hadoop
```

### ะะฐะฟััะบ ะฟัะพะตะบัะฐ

```bash
# ะัะปะธ ะบะพะฝัะตะนะฝะตั ัะถะต ัะพะฑัะฐะฝ
docker compose up -d

# ะะพะดะบะปััะธัััั ะบ ะบะพะฝัะตะนะฝะตัั
docker compose exec hadoop bash

# Hadoop ะทะฐะฟััะบะฐะตััั ะฐะฒัะพะผะฐัะธัะตัะบะธ ะฟัะธ ััะฐััะต ะบะพะฝัะตะนะฝะตัะฐ
# ะัะพะฒะตัะธัั ััะฐััั
jps
```

### ะัะธััะบะฐ ะฟัะพะตะบัะฐ

```bash
# ะััะฐะฝะพะฒะธัั ะธ ัะดะฐะปะธัั ะบะพะฝัะตะนะฝะตั
docker compose down

# ะฃะดะฐะปะธัั ะบะพะฝัะตะนะฝะตั + volumes (ะฒัะต ะดะฐะฝะฝัะต HDFS)
docker compose down -v

# ะฃะดะฐะปะธัั ะพะฑัะฐะท
docker rmi hadoop-earthquake:latest

# ะะพะปะฝะฐั ะพัะธััะบะฐ (ะฒัะต Docker ัะตััััั)
docker system prune -a
```

---

## ๐ ะะตะฑ-ะธะฝัะตััะตะนัั

ะะพัะปะต ะทะฐะฟััะบะฐ ะบะพะฝัะตะนะฝะตัะฐ ะดะพัััะฟะฝั:

- **HDFS NameNode:** http://localhost:9870
- **YARN:** http://localhost:8088
- **Jupyter:** http://localhost:8888 (ะฑะตะท ัะพะบะตะฝะฐ)

---

## โ FAQ

### Q: Hadoop ะฝะต ะทะฐะฟััะบะฐะตััั
**A:** ะะตัะตัะพะฑะตัะธัะต ะบะพะฝัะตะนะฝะตั:
```bash
docker compose down
docker compose build --no-cache
docker compose up -d
```

### Q: ะะตะฑ-ะธะฝัะตััะตะนัั ะฝะต ะพัะบััะฒะฐัััั
**A:** ะะพะฝัะธะณััะฐัะธั ะธัะฟะพะปัะทัะตั `0.0.0.0` ะดะปั ะดะพัััะฟะฐ ะธะทะฒะฝะต. ะะตัะตัะพะฑะตัะธัะต ะบะพะฝัะตะนะฝะตั (ัะผ. ะฒััะต).

### Q: DataNode ะฝะต ะทะฐะฟััะบะฐะตััั
**A:** ะัะพะฒะตัััะต ัะฐะนะป `hadoop/workers`:
```bash
cat hadoop/workers  # ะดะพะปะถะฝะพ ะฑััั: localhost
sed -i 's/\r$//' hadoop/workers  # ะธัะฟัะฐะฒะธัั ะพะบะพะฝัะฐะฝะธั ัััะพะบ
```

### Q: Jupyter ะฝะต ะพัะบััะฒะฐะตััั
**A:** ะะฐะฟัััะธัะต Jupyter ะฒะฝัััะธ ะบะพะฝัะตะนะฝะตัะฐ:
```bash
docker compose exec hadoop bash
cd /opt
bash scripts/start_jupyter.sh
# ะัะบัะพะนัะต http://localhost:8888 ะฒ ะฑัะฐัะทะตัะต
```

---

## ๐ ะะธัะตะฝะทะธั

ะฃัะตะฑะฝัะน ะฟัะพะตะบั ะดะปั ะปะฐะฑะพัะฐัะพัะฝะพะน ัะฐะฑะพัั.

---

## ๐ฏ ะัะพะณะพะฒะฐั ะฟัะพะฒะตัะบะฐ

ะะตัะตะด ะทะฐะฒะตััะตะฝะธะตะผ ัะฐะฑะพัั ัะฑะตะดะธัะตัั, ััะพ ะฒัะฟะพะปะฝะตะฝั ะฒัะต ัะฐะณะธ:

### โ ะงะตะบะปะธัั ะฒัะฟะพะปะฝะตะฝะธั

- [ ] Docker ะบะพะฝัะตะนะฝะตั ะทะฐะฟััะตะฝ ะธ ัะฐะฑะพัะฐะตั
- [ ] ะัะต ัะตัะฒะธัั Hadoop ะทะฐะฟััะตะฝั (ะฟัะพะฒะตัะธัั ัะตัะตะท `jps`)
- [ ] ะะฐะฝะฝัะต ะทะฐะณััะถะตะฝั ะฒ HDFS (`/user/hadoop/input/database.csv`)
- [ ] ะะตะฑ-ะธะฝัะตััะตะนัั ะพัะบััะฒะฐัััั ะฒ ะฑัะฐัะทะตัะต
- [ ] ะะฝะฐะปะธะท ะฒัะฟะพะปะฝะตะฝ ัะตัะตะท Pandas ะธะปะธ Spark
- [ ] ะะตะทัะปััะฐัั ัะพััะฐะฝะตะฝั ะฒ `results/`
- [ ] Jupyter notebook ะฒัะฟะพะปะฝะตะฝ, ัะพะทะดะฐะฝั ะฒะธะทัะฐะปะธะทะฐัะธะธ
- [ ] Git-ัะตะฟะพะทะธัะพัะธะน ัะพะทะดะฐะฝ ั ะฟะพะปะฝัะผ ะพััะตัะพะผ
- [ ] ะัะต ัะบัะธะฝัะพัั ะฒะตะฑ-ะธะฝัะตััะตะนัะพะฒ ะดะพะฑะฐะฒะปะตะฝั
- [ ] ะััะตั ัะพะพัะฒะตัััะฒัะตั ะบัะธัะตัะธัะผ ะพัะตะฝะบะธ

### ๐ฆ ะกะพะทะดะฐะฝะธะต Git-ัะตะฟะพะทะธัะพัะธั

```bash
# ะะฝะธัะธะฐะปะธะทะธัะพะฒะฐัั ัะตะฟะพะทะธัะพัะธะน
git init

# ะะพะฑะฐะฒะธัั ะฒัะต ัะฐะนะปั
git add .

# ะกะพะทะดะฐัั ะฟะตัะฒัะน ะบะพะผะผะธั
git commit -m "Initial commit: Hadoop Earthquake Analysis Project"

# ะะพะฑะฐะฒะธัั ัะดะฐะปะตะฝะฝัะน ัะตะฟะพะทะธัะพัะธะน (GitHub/GitLab)
git remote add origin <your-repository-url>

# ะะฐะณััะทะธัั ะธะทะผะตะฝะตะฝะธั
git push -u origin main
```

### ๐ ะัะพะฒะตัะบะฐ ัะตะทัะปััะฐัะพะฒ

```bash
# ะัะพะฒะตัะธัั ะฝะฐะปะธัะธะต ัะตะทัะปััะฐัะพะฒ
ls -lh results/

# ะัะพัะผะพััะตัั ัะตะทัะปััะฐัั
cat results/magnitude_by_type.csv
head -5 results/report.txt

# ะัะพะฒะตัะธัั ะฒะตะฑ-ะธะฝัะตััะตะนัั
echo "ะัะบัะพะนัะต ะฒ ะฑัะฐัะทะตัะต:"
echo "  HDFS: http://localhost:9870"
echo "  YARN: http://localhost:8088"
echo "  Jupyter: http://localhost:8888"
```

---

**ะัะพะตะบั ะณะพัะพะฒ ะบ ะธัะฟะพะปัะทะพะฒะฐะฝะธั! ๐**


